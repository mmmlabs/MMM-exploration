{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eebe26-1c5a-4176-82a4-b4e7bafd88b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install rpy2 to allow R integration within a Python env\n",
    "!pip install --upgrade rpy2\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "!pip install nevergrad\n",
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70de4e-ac3f-4c7f-8b8b-d87877e82a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Clear Workspace\n",
    "rm(list=ls())\n",
    "\n",
    "# Define Data Path\n",
    "root        <- \"/home/jupyter/mmm\"\n",
    "data_path   <- \"Data\"\n",
    "models_path <- \"Models\"\n",
    "\n",
    "# Define Input File Name\n",
    "data.raw   <- \"BA.xlsx\"\n",
    "\n",
    "# Sheets within The Data\n",
    "sales.sheet  <- \"sales\"\n",
    "ads.sheet    <- \"media\"\n",
    "others.sheet <- \"macros\"\n",
    "\n",
    "# Load Libraries\n",
    "# Load in the order that you want to latest function to exists\n",
    "library(readxl)\n",
    "library(ggplot2)\n",
    "library(scales)\n",
    "library(lubridate)\n",
    "\n",
    "install.packages(\"reticulate\", dependencies = TRUE, repos='http://cran.rstudio.com/')\n",
    "\n",
    "library(reticulate)\n",
    "Sys.setenv(RETICULATE_PYTHON = \"/opt/conda/bin/python3\")\n",
    "\n",
    "library(remotes)\n",
    "library(tidyverse)\n",
    "\n",
    "install.packages(\"minpack.lm\")\n",
    "install.packages(\"nloptr\")\n",
    "\n",
    "install.packages(\"Robyn\", dependencies = TRUE)\n",
    "library(Robyn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8ffd0-e03d-421b-8ff8-fd86a3f5c1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Read Offline Data\n",
    "setwd(paste(root))\n",
    "robyn_directory <- paste(root, models_path, sep = \"/\")\n",
    "\n",
    "sales.raw  <- read_excel(data.raw, sheet = sales.sheet)\n",
    "ads.raw    <- read_excel(data.raw, sheet = ads.sheet)\n",
    "macros.raw <- read_excel(data.raw, sheet = others.sheet)\n",
    "\n",
    "# Clean-up\n",
    "rm(data.raw, sales.sheet, ads.sheet, others.sheet)\n",
    "\n",
    "# Convert dates\n",
    "# Ensure the \"Week Ending Sat\" column is of the same type (Date) in all dataframes\n",
    "sales.raw$`Week Ending Sat` <- as.Date(sales.raw$`Week Ending Sat`)\n",
    "ads.raw$`Week Ending Sat` <- as.Date(ads.raw$`Week Ending Sat`)\n",
    "macros.raw$`Week Ending Sat` <- as.Date(macros.raw$`Week Ending Sat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc193a0-0c14-4a46-be25-d9a561bf6333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Run summary statistics\n",
    "# Business validation\n",
    "\n",
    "# Summary of sales numbers (dollars, units, ASP) & comparison to last year\n",
    "\n",
    "# Find the latest date in your data\n",
    "latest_date <- max(sales.raw$`Week Ending Sat`)\n",
    "\n",
    "# Separate data frames for latest 52 weeks and prior 52 weeks\n",
    "latest_52_weeks <- sales.raw %>%\n",
    "  filter(`Week Ending Sat` > latest_date - weeks(52) & `Week Ending Sat` <= latest_date)\n",
    "\n",
    "prior_52_weeks <- sales.raw %>%\n",
    "  filter(`Week Ending Sat` > latest_date - weeks(104) & `Week Ending Sat` <= latest_date - weeks(52))\n",
    "\n",
    "# Summarize data\n",
    "summarize_data <- function(data) {\n",
    "  data %>%\n",
    "    group_by(`Sales Source`) %>%\n",
    "    summarize(Sales = sum(`Net Revenue (Sales)`),\n",
    "              Non_Zero_Obs = sum(`Net Revenue (Sales)` != 0),\n",
    "              .groups = 'drop')\n",
    "}\n",
    "\n",
    "# Apply summarization function\n",
    "summary_latest = summarize_data(latest_52_weeks)\n",
    "summary_prior = summarize_data(prior_52_weeks)\n",
    "\n",
    "# Join the summaries and calculate percentage delta\n",
    "summary_table <- full_join(summary_prior, summary_latest, by = \"Sales Source\", suffix = c(\"_Prior\", \"_Latest\")) %>%\n",
    "  mutate(Pct_Delta = (Sales_Latest - Sales_Prior) / Sales_Prior * 100)\n",
    "\n",
    "# Arrange columns in the specified order\n",
    "summary_table <- summary_table %>%\n",
    "  select(`Sales Source`,\n",
    "         Sales_Prior, Sales_Latest, Pct_Delta,\n",
    "         Non_Zero_Obs_Prior, Non_Zero_Obs_Latest)\n",
    "\n",
    "# Print the summary table\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037cbf73-003d-4ae7-a8ff-3ee320f5a39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Creating the plot\n",
    "ggplot(sales.raw, aes(x = `Week Ending Sat`, y = `Net Revenue (Sales)`)) +\n",
    "  geom_line() + # Line plot\n",
    "  facet_grid(`Sales Source` ~ ., scales = \"free_y\") + # Facet by Source\n",
    "  scale_y_continuous(labels = scales::label_number(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n",
    "  theme_minimal() + # Minimal theme\n",
    "  labs(title = \"Net Revenue (Sales)\",\n",
    "       x = \"Week Ending Saturday\",\n",
    "       y = \"Net Revenue (Sales)\") +\n",
    "  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41a907-ed2d-4690-87f4-806fc5705dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Summary of advertising\n",
    "\n",
    "# Arrange the data by \"Week Ending Sat\"\n",
    "ads.raw <- ads.raw %>%\n",
    "  arrange(`Week Ending Sat`)\n",
    "\n",
    "# Create a complete sequence of weeks\n",
    "all_weeks <- seq(from = min(ads.raw$`Week Ending Sat`), to = max(ads.raw$`Week Ending Sat`), by = \"week\")\n",
    "\n",
    "# Expand the dataset to include all combinations of weeks, \"Ad Type\", and Media Group\n",
    "ads.complete <- ads.raw %>%\n",
    "  complete(`Week Ending Sat` = all_weeks, `Ad Type`, `Media Group`, fill = list(`Ad Spend` = 0))\n",
    "\n",
    "# Creating the plot\n",
    "ggplot(ads.complete, aes(x = `Week Ending Sat`, y = `Ad Spend`)) +\n",
    "  geom_line() + # Line plot\n",
    "  facet_grid(`Ad Type` ~ ., scales = \"free_y\", space = \"free\") + # Facet by \"Ad Type\"\n",
    "  theme_minimal() + # Minimal theme\n",
    "  labs(title = \"Ad Spend Over Time by Ad Type\",\n",
    "       x = \"Week Ending Saturday\",\n",
    "       y = \"Ad Spend\") +\n",
    "  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199246ce-2587-4311-922a-2c1ccff6f7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Merge all raw data frames\n",
    "# Pivot data\n",
    "# Pivot the ads.raw data\n",
    "ads.raw.wide <- ads.raw %>%\n",
    "  select(`Week Ending Sat`, varname, `Ad Spend`, Impressions, Clicks, `Ad Attributed Sales on AMA`, `Ad Units from Kantar`) %>%\n",
    "  pivot_wider(names_from = varname,\n",
    "              values_from = c(`Ad Spend`, Impressions, Clicks, `Ad Attributed Sales on AMA`, `Ad Units from Kantar`),\n",
    "              names_glue = \"{varname}_{.value}\") %>%\n",
    "  arrange(`Week Ending Sat`)\n",
    "\n",
    "# Merge the data frames\n",
    "modelling_data <- sales.raw %>%\n",
    "  left_join(ads.raw.wide, by = \"Week Ending Sat\") %>%\n",
    "  left_join(macros.raw, by = \"Week Ending Sat\")\n",
    "\n",
    "# set the dates to Sunday because of Robyn's inflexiability\n",
    "modelling_data$`Week Ending Sun` <- modelling_data$`Week Ending Sat` + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238f0b2-0ff3-4920-80b5-07b9415002e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Missing values\n",
    "modelling_data[is.na(modelling_data)] <- 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a10b5-3d7e-41cb-abe5-e79629a27ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Remove spaces\n",
    "names(modelling_data) <- gsub(\" \", \"_\", names(modelling_data))\n",
    "\n",
    "# View the merged data frame\n",
    "head(modelling_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99922f6-017e-410c-a613-f2370cad7797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Separate out each Sales Source into it's own dataframe\n",
    "# Create the first dataframe for \"Source1\"\n",
    "modeling_data_ama <- modelling_data %>%\n",
    "  filter(Sales_Source == \"AMA\")\n",
    "\n",
    "# Create the second dataframe for \"Source2\"\n",
    "modeling_data_others <- modelling_data %>%\n",
    "  filter(Sales_Source == \"IRI\")\n",
    "\n",
    "# End Read & Validate Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5080cfd-1ae4-4eaf-ad27-13b58fff980a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "packageVersion(\"Robyn\")\n",
    "\n",
    "## Force multi-core use when running RStudio\n",
    "Sys.setenv(R_FUTURE_FORK_ENABLE = \"true\")\n",
    "options(future.fork.enable = TRUE)\n",
    "\n",
    "# Set to FALSE to avoid the creation of files locally\n",
    "create_files <- TRUE\n",
    "\n",
    "## All sign control are now automatically provided: \"positive\" for media & organic\n",
    "## variables and \"default\" for all others. User can still customise signs if necessary.\n",
    "## Documentation is available, access it anytime by running: ?robyn_inputs\n",
    "modeling_data_ama <- modeling_data_ama %>%\n",
    "  rename(Net_Revenue_Sales = `Net_Revenue_(Sales)`)\n",
    "\n",
    "paid_media_spends = names(modeling_data_ama)[grepl(\"_Ad_Spend$\", names(modeling_data_ama))]\n",
    "paid_media_spends <- paid_media_spends[paid_media_spends != \"mobileApp_Ad_Spend\"]\n",
    "\n",
    "paid_media_vars = names(modeling_data_ama)[grepl(\"_Ad_Spend$\", names(modeling_data_ama))]\n",
    "paid_media_vars <- paid_media_vars[paid_media_vars != \"mobileApp_Ad_Spend\"]\n",
    "\n",
    "modeling_data_ama$Avg_Sales_Price <-\n",
    "  modeling_data_ama$Net_Revenue_Sales / modeling_data_ama$Sold_Units\n",
    "\n",
    "InputCollect <- robyn_inputs(\n",
    "  dt_input = modeling_data_ama,\n",
    "  #dt_holidays = dt_prophet_holidays,\n",
    "  date_var = \"Week_Ending_Sun\", # date format must be \"2020-01-01\"\n",
    "  dep_var = \"Net_Revenue_Sales\", # there should be only one dependent variable\n",
    "  dep_var_type = \"revenue\", # \"revenue\" (ROI) or \"conversion\" (CPA)\n",
    "  prophet_vars = c(\"trend\", \"season\", \"holiday\"), # \"trend\",\"season\", \"weekday\" & \"holiday\"\n",
    "  prophet_country = \"US\", # input country code. Check: dt_prophet_holidays\n",
    "  context_vars = c(\"Avg_Sales_Price\", \"Total_Organic_Impressions_On_AMA\"), # e.g. competitors, discount, unemployment etc\n",
    "  paid_media_spends = paid_media_spends, # mandatory input\n",
    "  paid_media_vars = paid_media_vars, # mandatory.\n",
    "  # paid_media_vars must have same order as paid_media_spends. Use media exposure metrics like\n",
    "  # impressions, GRP etc. If not applicable, use spend instead.\n",
    "  #organic_vars = \"Avg_Sales_Price\", # marketing activity without media spend\n",
    "  # factor_vars = c(\"events\"), # force variables in context_vars or organic_vars to be categorical\n",
    "  window_start = min(modeling_data_ama$Week_Ending_Sun),\n",
    "  window_end = max(modeling_data_ama$Week_Ending_Sun),\n",
    "  adstock = \"geometric\" # geometric, weibull_cdf or weibull_pdf.\n",
    ")\n",
    "print(InputCollect)\n",
    "\n",
    "#### 2a-2: Second, define and add hyperparameters\n",
    "\n",
    "## Default media variable for modelling has changed from paid_media_vars to paid_media_spends.\n",
    "## Also, calibration_input are required to be spend names.\n",
    "## hyperparameter names are based on paid_media_spends names too. See right hyperparameter names:\n",
    "hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)\n",
    "\n",
    "## Guide to setup & understand hyperparameters\n",
    "\n",
    "## Robyn's hyperparameters have four components:\n",
    "## - Adstock parameters (theta or shape/scale)\n",
    "## - Saturation parameters (alpha/gamma)\n",
    "## - Regularisation parameter (lambda). No need to specify manually\n",
    "## - Time series validation parameter (train_size)\n",
    "\n",
    "## 1. IMPORTANT: set plot = TRUE to create example plots for adstock & saturation\n",
    "## hyperparameters and their influence in curve transformation.\n",
    "plot_adstock(plot = F)\n",
    "plot_saturation(plot = F)\n",
    "\n",
    "## 2. Get correct hyperparameter names:\n",
    "# All variables in paid_media_spends and organic_vars require hyperparameter and will be\n",
    "# transformed by adstock & saturation.\n",
    "# Run hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)\n",
    "# to get correct media hyperparameter names. All names in hyperparameters must equal\n",
    "# names from hyper_names(), case sensitive. Run ?hyper_names to check function arguments.\n",
    "\n",
    "## 3. Hyperparameter interpretation & recommendation:\n",
    "\n",
    "## Geometric adstock: Theta is the only parameter and means fixed decay rate. Assuming TV\n",
    "# spend on day 1 is 100€ and theta = 0.7, then day 2 has 100*0.7=70€ worth of effect\n",
    "# carried-over from day 1, day 3 has 70*0.7=49€ from day 2 etc. Rule-of-thumb for common\n",
    "# media genre: TV c(0.3, 0.8), OOH/Print/Radio c(0.1, 0.4), digital c(0, 0.3). Also,\n",
    "# to convert weekly to daily we can transform the parameter to the power of (1/7),\n",
    "# so to convert 30% daily to weekly is 0.3^(1/7) = 0.84.\n",
    "\n",
    "## 4. Set individual hyperparameter bounds. They either contain two values e.g. c(0, 0.5),\n",
    "# or only one value, in which case you'd \"fix\" that hyperparameter.\n",
    "# Run hyper_limits() to check maximum upper and lower bounds by range\n",
    "hyper_limits()\n",
    "\n",
    "# Merge the vectors\n",
    "hyperparameters_ranges <- list(\"alphas\" = c(0.5, 3),\n",
    "                               \"gammas\" = c(0.3, 1),\n",
    "                               \"thetas\" = c(0, 0.3))\n",
    "\n",
    "# Merge the list of ranges with the variable names\n",
    "hyperparameters <- lapply(paid_media_vars, function(v) {\n",
    "  setNames(\n",
    "    lapply(names(hyperparameters_ranges), function(n) hyperparameters_ranges[[n]]),\n",
    "    paste(v, names(hyperparameters_ranges), sep = \"_\")\n",
    "  )\n",
    "})\n",
    "\n",
    "hyperparameters <- do.call(c, hyperparameters)\n",
    "\n",
    "hyperparameters <- c(hyperparameters, list('train_size' = c(0.5, 0.8)))\n",
    "\n",
    "#### 2a-3: Third, add hyperparameters into robyn_inputs()\n",
    "InputCollect <- robyn_inputs(InputCollect = InputCollect, hyperparameters = hyperparameters)\n",
    "print(InputCollect)\n",
    "\n",
    "#### Check spend exposure fit if available\n",
    "if (length(InputCollect$exposure_vars) > 0) {\n",
    "  lapply(InputCollect$modNLS$plots, plot)\n",
    "}\n",
    "\n",
    "\n",
    "################################################################\n",
    "#### Step 3: Build initial model\n",
    "\n",
    "## Run all trials and iterations. Use ?robyn_run to check parameter definition\n",
    "OutputModels <- robyn_run(\n",
    "  InputCollect = InputCollect, # feed in all model specification\n",
    "  cores = NULL, # NULL defaults to (max available - 1)\n",
    "  iterations = 1000, # 2000 recommended for the dummy dataset with no calibration\n",
    "  trials = 3, # 5 recommended for the dummy dataset\n",
    "  ts_validation = FALSE, # 3-way-split time series for NRMSE validation.\n",
    "  add_penalty_factor = FALSE # Experimental feature. Use with caution.\n",
    ")\n",
    "print(OutputModels)\n",
    "\n",
    "## Check MOO (multi-objective optimization) convergence plots\n",
    "# Read more about convergence rules: ?robyn_converge\n",
    "OutputModels$convergence$moo_distrb_plot\n",
    "OutputModels$convergence$moo_cloud_plot\n",
    "\n",
    "## Check time-series validation plot (when ts_validation == TRUE)\n",
    "# Read more and replicate results: ?ts_validation\n",
    "if (OutputModels$ts_validation) OutputModels$ts_validation_plot\n",
    "\n",
    "## Calculate Pareto fronts, cluster and export results and plots. See ?robyn_outputs\n",
    "OutputCollect <- robyn_outputs(\n",
    "  InputCollect, OutputModels,\n",
    "  pareto_fronts = \"auto\", # automatically pick how many pareto-fronts to fill min_candidates (100)\n",
    "  # min_candidates = 100, # top pareto models for clustering. Default to 100\n",
    "  # calibration_constraint = 0.1, # range c(0.01, 0.1) & default at 0.1\n",
    "  csv_out = \"pareto\", # \"pareto\", \"all\", or NULL (for none)\n",
    "  clusters = TRUE, # Set to TRUE to cluster similar models by ROAS. See ?robyn_clusters\n",
    "  export = create_files, # this will create files locally\n",
    "  plot_folder = robyn_directory, # path for plots exports and files creation\n",
    "  plot_pareto = create_files # Set to FALSE to deactivate plotting and saving model one-pagers\n",
    ")\n",
    "print(OutputCollect)\n",
    "\n",
    "################################################################\n",
    "#### Step 4: Select and save the any model\n",
    "\n",
    "## Compare all model one-pagers and select one that mostly reflects your business reality\n",
    "print(OutputCollect)\n",
    "\n",
    "sort(OutputCollect$allSolutions)\n",
    "OutputCollect$clusters$models$solID\n",
    "OutputCollect$clusters$models$mape\n",
    "OutputCollect$clusters$models$decomp.rssd\n",
    "OutputCollect$clusters$models$nrmse\n",
    "OutputCollect$clusters$models$nrmse_test\n",
    "OutputCollect$clusters$models$nrmse_train\n",
    "# Print summary stats of all the Pareto models\n",
    "\n",
    "select_model <- \"2_64_9\" # Pick one of the models from OutputCollect to proceed\n",
    "\n",
    "#### Version >=3.7.1: JSON export and import (faster and lighter than RDS files)\n",
    "ExportedModel <- robyn_write(InputCollect, OutputCollect, select_model, export = create_files)\n",
    "print(ExportedModel)\n",
    "\n",
    "ExportedModel$ExportedModel$errors$rsq_train\n",
    "\n",
    "# To plot any model's one-pager:\n",
    "myOnePager <- robyn_onepagers(InputCollect, OutputCollect, select_model, export = FALSE)\n",
    "\n",
    "# To check each of the one-pager's plots\n",
    "myOnePager[[select_model]]$patches$plots[[1]]\n",
    "myOnePager[[select_model]]$patches$plots[[2]]\n",
    "myOnePager[[select_model]]$patches$plots[[3]]\n",
    "myOnePager[[select_model]]$patches$plots[[4]]\n",
    "myOnePager[[select_model]]$patches$plots[[5]]\n",
    "myOnePager[[select_model]]$patches$plots[[6]]\n",
    "myOnePager[[select_model]]$patches$plots[[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4f453-2ac8-4b01-a7a0-d5d1c37903db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
